---
permalink: /
title: "About Me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am an AI researcher specializing in speech processing, reinforcement learning, and large-scale dataset creation, driven by a fundamental belief: the most challenging problems in machine learning stem from data scarcity. During my master's studies at [Sharif University of Technology](https://sharif.edu) under [Professor Babak Khalaj](https://sharif.edu/~khalaj/), I confronted a particularly complex data-scarce scenario in edge computing, where environmental properties changed rapidly and optimization problems proved NP-hard. I developed a hybrid meta-reinforcement learning framework that leveraged experiences across multiple environments, enabling robust adaptation despite limited data availability. This work established my foundation in both reinforcement learning and the critical importance of data-centric approaches to solving hard problems.

After graduating, I joined the Foundation AI Models Lab at Sharif University under [Dr. Sajjad Amini](https://sharif.edu/~s_amini/) and [Dr. Shahrokh Ghaemmaghami](https://sharif.edu/~ghaemmag/), where I encountered an even more pressing data challenge: building speech processing capabilities for Persian, a severely under-resourced language. With only Common Voice's 400 hours of labeled audio available, I recognized that achieving state-of-the-art performance would require fundamentally better data infrastructure. I architected two scalable pipelines for automated speech dataset creation and led teams to collect 11.8k hours of raw audio-text pairs spanning diverse domains. Through my [CTC-segmentation toolkit](https://github.com/saeedzou/ctc-segmentation-toolkit), I refined this into 7.4k hours of aligned, validated data—a 19.75× improvement over existing resources. This infrastructure enabled breakthrough results: I trained three ASR models that achieved 4.59% WER with [Parakeet](https://huggingface.co/spaces/saeedzou/persian_asr_nemo), a 78% relative improvement over initial baselines. From 600 hours of cleaned audiobook data, I fine-tuned F5-TTS, the first bilingual Persian-English zero-shot TTS model. I also fine-tuned seed-VC on 1,000 hours of curated Persian audiobooks, creating the current state-of-the-art voice conversion system. These advances motivated **PersianVox**, my ongoing project to release the largest in-the-wild speech corpus for Persian, democratizing access for researchers working on under-resourced languages.

Beyond academic research, I serve as an AI consultant at Bimeh Dot Com, where I've built document understanding pipelines that extract structured information from complex PDFs using layout-aware models and LLMs. I'm currently developing reinforcement learning applications for quantitative finance, specifically designing trading agents with sophisticated risk management capabilities—including dynamic stop-loss mechanisms—an area often overlooked in financial ML research. 

My work spans the full spectrum of modern AI: from data infrastructure and model training to real-world deployment, always focused on solving hard problems that create tangible impact for research communities, and practical applications.
